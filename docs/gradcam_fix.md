# Fix GradCAM - RÃ©solution du problÃ¨me de variabilitÃ© des heatmaps

**Date:** 24 octobre 2024  
**ProblÃ¨me:** Les heatmaps GradCAM semblaient similaires d'une image Ã  l'autre, suggÃ©rant une pollution des gradients ou un problÃ¨me de cache.

## ğŸ”´ ProblÃ¨mes identifiÃ©s

### 1. Pollution des gradients entre les analyses successives

**Localisation:** `src/utils/gradcam_analyzer.py`, fonction `gradcam_analysis()`

**ProblÃ¨me:**
```python
# Code original - PROBLÃ‰MATIQUE
def gradcam_analysis(...):
    image = Image.open(image_path).convert("RGB")
    input_tensor = transform(image).unsqueeze(0)
    
    # Pas de nettoyage des gradients
    output = model(input_tensor)
    probs = F.softmax(output, dim=1)
    
    # GradCAM sans rÃ©initialisation
    gradcam = LayerGradCam(model, target_layer)
    attribution = gradcam.attribute(input_tensor, target=pred_class)
```

**ConsÃ©quences:**
- Les gradients s'accumulent entre les appels successifs
- Les hooks de Captum peuvent laisser des Ã©tats rÃ©siduels
- Les heatmaps deviennent de plus en plus similaires

### 2. Cache Streamlit trop agressif

**Localisation:** `src/pages/5_Ameliorations_potentielles.py`, fonction `load_model()`

**ProblÃ¨me:**
```python
# Code original - PROBLÃ‰MATIQUE
@st.cache_resource
def load_model(checkpoint_file: str) -> object:
    return load_resnet_model(checkpoint_path, num_classes=8)

model = load_model(checkpoint_filename)
```

**ConsÃ©quences:**
- Le modÃ¨le est chargÃ© une seule fois et rÃ©utilisÃ© pour toutes les images
- L'Ã©tat interne du modÃ¨le n'est jamais rÃ©initialisÃ©
- Les gradients rÃ©siduels persistent entre les analyses

### 3. Absence de gestion du contexte torch

**ProblÃ¨me:**
- Pas de `torch.no_grad()` pour l'infÃ©rence (gaspillage de mÃ©moire)
- Pas de `model.zero_grad()` entre les analyses
- Pas de `.cpu()` pour libÃ©rer la mÃ©moire GPU

### 4. Division par zÃ©ro potentielle

**ProblÃ¨me:**
```python
# Code original - RISQUE
heatmap /= heatmap.max()
```

Si `heatmap.max() == 0`, cela provoque une division par zÃ©ro.

## âœ… Solutions implÃ©mentÃ©es

### 1. Nettoyage explicite des gradients

**Fichier:** `src/utils/gradcam_analyzer.py`

```python
def gradcam_analysis(...):
    # Ensure model is in eval mode and gradients are cleared
    model.eval()
    model.zero_grad()
    
    image = Image.open(image_path).convert("RGB")
    input_tensor = transform(image).unsqueeze(0)
    
    # Prediction with no_grad context for efficiency
    with torch.no_grad():
        output = model(input_tensor)
        probs = F.softmax(output, dim=1)
        pred_class = probs.argmax(dim=1).item()
        pred_conf = probs[0, pred_class].item() * 100
    
    # Clear any residual gradients before GradCAM
    model.zero_grad()
    
    # Grad-CAM computation (requires gradients)
    target_layer = model.layer4[-1]
    gradcam = LayerGradCam(model, target_layer)
    
    # Enable gradients only for attribution
    input_tensor.requires_grad = True
    attribution = gradcam.attribute(input_tensor, target=pred_class)
    
    # Clean up gradients after attribution
    model.zero_grad()
    
    upsampled_attr = LayerAttribution.interpolate(attribution, input_tensor.shape[2:])
    heatmap = upsampled_attr.squeeze().detach().cpu().numpy()
    heatmap = np.maximum(heatmap, 0)
    
    # Avoid division by zero
    if heatmap.max() > 0:
        heatmap /= heatmap.max()
    else:
        heatmap = np.zeros_like(heatmap)
```

**AmÃ©liorations:**
- âœ… `model.zero_grad()` avant et aprÃ¨s chaque analyse
- âœ… `torch.no_grad()` pour l'infÃ©rence (Ã©conomie de mÃ©moire)
- âœ… `.cpu()` pour libÃ©rer la mÃ©moire GPU
- âœ… Protection contre la division par zÃ©ro
- âœ… `input_tensor.requires_grad = True` explicite pour GradCAM

### 2. AmÃ©lioration du cache Streamlit

**Fichier:** `src/pages/5_Ameliorations_potentielles.py`

```python
# Load model with proper caching
@st.cache_resource
def load_model(checkpoint_file: str) -> object:
    yaml_loader = YamlLoader()
    checkpoint_dir = yaml_loader.get_nested_value(
        'paths.models.checkpoints', 
        './models/checkpoints'
    )
    checkpoint_path = Path(checkpoint_dir) / checkpoint_file
    model = load_resnet_model(checkpoint_path, num_classes=8)
    # Ensure clean state
    model.eval()
    model.zero_grad()
    return model

model = load_model(checkpoint_filename)
transform = get_image_transform()

# Add option to clear cache if needed
if st.sidebar.button(f"ğŸ”„ Recharger le modÃ¨le ({tab_title[:20]}...)", key=f"reload_{checkpoint_filename}"):
    st.cache_resource.clear()
    st.rerun()
```

**AmÃ©liorations:**
- âœ… Initialisation propre du modÃ¨le dans le cache
- âœ… Bouton pour forcer le rechargement du modÃ¨le
- âœ… Nettoyage du cache Streamlit

### 3. Gestion du sampling alÃ©atoire

```python
if selected_class and class_images.get(selected_class):
    # Use a seed for reproducibility but allow refresh
    if st.button("ğŸ² Nouvelles images alÃ©atoires", key=f"refresh_{checkpoint_filename}"):
        st.session_state[f"seed_{checkpoint_filename}"] = random.randint(0, 10000)
    
    seed = st.session_state.get(f"seed_{checkpoint_filename}", 42)
    random.seed(seed)
    
    sample_images = random.sample(
        class_images[selected_class],
        min(3, len(class_images[selected_class]))
    )
    st.info(
        f"Analyse Grad-CAM sur **3 images alÃ©atoires** "
        f"de la classe **{selected_class}** (seed: {seed})."
    )
```

**AmÃ©liorations:**
- âœ… Seed reproductible pour le debugging
- âœ… Bouton pour rafraÃ®chir les images
- âœ… Affichage du seed pour la traÃ§abilitÃ©

## ğŸ“Š Impact attendu

### Avant les corrections:
- Heatmaps trÃ¨s similaires entre images diffÃ©rentes
- Accumulation de gradients rÃ©siduels
- Comportement non dÃ©terministe

### AprÃ¨s les corrections:
- âœ… Heatmaps distinctes pour chaque image
- âœ… Pas d'accumulation de gradients
- âœ… Comportement reproductible (avec seed)
- âœ… Meilleure utilisation de la mÃ©moire
- âœ… Robustesse accrue (protection division par zÃ©ro)

## ğŸ§ª Tests recommandÃ©s

Pour vÃ©rifier que les corrections fonctionnent:

1. **Test de variabilitÃ©:**
   ```bash
   uv run python test_gradcam_issue.py
   ```
   VÃ©rifier que:
   - Les heatmaps ont des distributions diffÃ©rentes
   - La corrÃ©lation entre heatmaps est faible (<0.8)
   - Les prÃ©dictions sont cohÃ©rentes

2. **Test dans Streamlit:**
   - SÃ©lectionner une classe
   - Observer 3 images diffÃ©rentes
   - Cliquer sur "ğŸ² Nouvelles images alÃ©atoires"
   - VÃ©rifier que les heatmaps changent significativement

3. **Test de reproductibilitÃ©:**
   - Noter le seed affichÃ©
   - Recharger la page
   - VÃ©rifier que les mÃªmes images produisent les mÃªmes heatmaps

## ğŸ” Diagnostic technique

### Pourquoi les gradients s'accumulaient?

1. **PyTorch conserve les gradients par dÃ©faut:**
   - Chaque forward pass calcule des gradients
   - Sans `zero_grad()`, ils s'additionnent

2. **Captum crÃ©e des hooks:**
   - LayerGradCam attache des hooks au modÃ¨le
   - Ces hooks peuvent laisser des Ã©tats rÃ©siduels

3. **Cache Streamlit:**
   - Le modÃ¨le est partagÃ© entre toutes les analyses
   - L'Ã©tat interne n'est jamais rÃ©initialisÃ©

### Pourquoi `.cpu()` est important?

```python
heatmap = upsampled_attr.squeeze().detach().cpu().numpy()
```

- `.detach()`: DÃ©tache le tensor du graphe de calcul
- `.cpu()`: TransfÃ¨re le tensor vers la RAM (libÃ¨re la VRAM)
- `.numpy()`: Convertit en numpy array

Sans `.cpu()`, les tensors restent en VRAM et peuvent causer des fuites mÃ©moire.

## ğŸ“š RÃ©fÃ©rences

- [Captum LayerGradCam Documentation](https://captum.ai/api/layer.html#layergradcam)
- [PyTorch Autograd Documentation](https://pytorch.org/docs/stable/autograd.html)
- [Streamlit Caching Best Practices](https://docs.streamlit.io/library/advanced-features/caching)

## ğŸ¯ Prochaines Ã©tapes

- [ ] Ajouter des tests unitaires pour `gradcam_analysis()`
- [ ] ImplÃ©menter d'autres mÃ©thodes d'interprÃ©tabilitÃ© (Integrated Gradients, LIME)
- [ ] Comparer les heatmaps entre modÃ¨les avec/sans masque
- [ ] Analyser les classes les plus confondues avec GradCAM
